# multi-llm

> **Status**: Phase 1 Complete - Extraction from myStory ‚úÖ
> **Next**: Phase 2 - Cleanup & Generalization (see [PHASE2_PLAN.md](PHASE2_PLAN.md))

Unified multi-provider LLM client with support for OpenAI, Anthropic, Ollama, and LMStudio.

## Current State

This library has been **successfully extracted** from the myStory project and is **~95% complete** as a standalone crate.

**What Works**:
- ‚úÖ All source code copied from mystory-llm
- ‚úÖ Core types extracted from mystory-core
- ‚úÖ Dependencies updated (no more mystory-core or mystory-logging)
- ‚úÖ Most imports updated
- ‚úÖ Logging module created

**What Needs Fixing**:
- ‚ö†Ô∏è ~43 compilation errors (mostly import path issues)
- ‚ö†Ô∏è Some unused imports to clean up
- ‚è≥ Tests not yet run

## Phase 2 Next Steps

**Immediate** (< 1 hour):
1. Fix remaining import errors (see [PHASE2_PLAN.md](PHASE2_PLAN.md) Step 1)
2. Clean up unused imports
3. Verify compilation: `cargo check`
4. Run tests: `cargo test --lib`

**Main Tasks** (1-2 weeks):
1. Refactor `core_types/` into proper public API modules
2. Review and improve error handling
3. Review business events system (make optional?)
4. Create comprehensive documentation and examples
5. Prepare for crates.io publication

See **[PHASE2_PLAN.md](PHASE2_PLAN.md)** for complete details.

## Key Features

- **Multiple Providers**: Seamless switching between OpenAI, Anthropic, Ollama, and LMStudio
- **Unified Messages**: Provider-agnostic message architecture with caching hints (core feature!)
- **Prompt Caching**: Native support for Anthropic prompt caching
- **Tool Calling**: First-class function/tool calling support
- **Resilience**: Built-in retry logic, rate limiting, and error handling

## Compatibility

- **Rust Edition**: 2021
- **MSRV**: Rust 1.75 or later
- **Edition Compatibility**: Works with projects using any Rust edition (2015, 2018, 2021, 2024)

## Project Structure

```
multi-llm/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core_types/        # Extracted types from mystory-core (Phase 2: refactor)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors.rs      # Error traits and types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ messages.rs    # ‚≠ê Unified message architecture (PRIMARY FEATURE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.rs    # Executor types and LLM provider trait
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ events.rs      # Business event logging
‚îÇ   ‚îú‚îÄ‚îÄ logging.rs         # Log macro re-exports (log_debug, log_error, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ client.rs          # UnifiedLLMClient
‚îÇ   ‚îú‚îÄ‚îÄ config.rs          # Configuration types
‚îÇ   ‚îú‚îÄ‚îÄ providers/         # Provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic/     # Anthropic Claude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.rs      # OpenAI GPT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.rs      # Ollama (local models)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lmstudio.rs    # LM Studio
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ tests/                 # Integration tests
‚îú‚îÄ‚îÄ EXTRACTION.md          # ‚úÖ Phase 1 completion summary
‚îú‚îÄ‚îÄ PHASE2_PLAN.md         # üìã Detailed Phase 2 tasks
‚îî‚îÄ‚îÄ PHASE3_PLAN.md         # üìã Integration back into myStory
```

## Documentation

- **[EXTRACTION.md](EXTRACTION.md)** - What was done in Phase 1, current state, notes for Phase 2
- **[PHASE2_PLAN.md](PHASE2_PLAN.md)** - Detailed plan for cleanup and generalization
- **[PHASE3_PLAN.md](PHASE3_PLAN.md)** - Plan for integrating back into myStory

## Getting Started (Phase 2)

```bash
# Navigate to the project
cd /Users/rick/git/multi-llm

# Read the context
cat EXTRACTION.md
cat PHASE2_PLAN.md

# Fix compilation errors (see PHASE2_PLAN.md Step 1)
# Then verify:
cargo check
cargo test --lib
```

## Architecture Highlights

### Unified Message Architecture

The core innovation of multi-llm is the **unified message** system that treats all providers consistently:

```rust
use multi_llm::{UnifiedMessage, MessageRole, MessageContent, MessageAttributes};

// Simple message
let msg = UnifiedMessage::user("Hello!");

// With caching hints (for Anthropic)
let system_msg = UnifiedMessage::system_instruction(
    "You are a helpful assistant",
    Some("system-v1".to_string())  // Cache key
);

// With priority ordering
let context_msg = UnifiedMessage::context(
    "User context...",
    Some("user-context".to_string())
);
```

### Provider Abstraction

```rust
use multi_llm::{UnifiedLLMClient, LLMConfig, OpenAIConfig};

let config = LLMConfig::openai(OpenAIConfig {
    api_key: "your-key".to_string(),
    model: "gpt-4".to_string(),
    ..Default::default()
});

let client = UnifiedLLMClient::new(config)?;
```

## Testing Strategy

**Unit Tests**: `cargo test --lib` (~2305 tests from mystory-llm)
**Integration Tests**: `cargo test --tests` (~107 tests, some require Docker)

Some integration tests are marked with `#[ignore]` and require external services:
```bash
# Run ignored tests
cargo test -- --ignored
```

## Origin

This library was extracted from the [myStory](../mystory) project to be a standalone, reusable multi-provider LLM client. The extraction was research-driven using Serena tools to identify minimal dependencies while maintaining semantic compatibility.

**Extraction Date**: 2025-01-21
**Original Crate**: `mystory-llm`
**Phase 1**: ‚úÖ Complete
**Phase 2**: üöß In Progress

## License

MIT OR Apache-2.0 (to be added in Phase 2)

## Contributing

Phase 2 is currently in progress. After completion, contribution guidelines will be added.

---

**For Phase 2 Contributors**: Start by reading [PHASE2_PLAN.md](PHASE2_PLAN.md) for detailed tasks and priorities.
